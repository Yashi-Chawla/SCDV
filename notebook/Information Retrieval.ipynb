{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22e11c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..\\scdv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7183874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "211c8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scdv import SCDV\n",
    "from baseline import BaselineEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c156ef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "scdv_model = SCDV.load(\"../saved_models/bbc_word2vec_sg_5_100.pkl\")\n",
    "baseline_model = SCDV.load(\"../saved_models/baseline_bbc_word2vec_sg_100.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5dbb919f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21475e31d1f443efa94071bb05a73ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5305c402d34df89defaa2ff5cabc82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"../data/bbc/all/\"\n",
    "\n",
    "documents = list()\n",
    "p = Path(data_path)\n",
    "files = list(p.glob(\"**/*.txt\"))\n",
    "for file in tqdm(files):\n",
    "    try:\n",
    "        with open(file, \"r\", encoding='utf8') as f:\n",
    "            text = f.read().strip()\n",
    "    except:\n",
    "        pass\n",
    "    documents.append(text)\n",
    "document_words = [word_tokenize(document) for document in tqdm(documents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82b98c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qd_score(model, query_idx, document_idx, query_unigram_probabilities, document_unigram_probabilities):\n",
    "    _sum = 0\n",
    "    for word in query_words[query_idx]:\n",
    "        _sum += query_unigram_probabilities[query_idx][word] * get_probability_wd(word, document_idx, document_unigram_probabilities, model)\n",
    "    return _sum\n",
    "\n",
    "def get_scores(model, query_idx, document_idx, query_unigram_probabilities, document_unigram_probabilities):\n",
    "    query_vector = query_vectors[query_idx]\n",
    "    document_vector = document_vectors[document_idx]\n",
    "    similarity = model.similarity(query_vector, document_vector)\n",
    "    score_qd = get_qd_score(model, query_idx, document_idx, query_unigram_probabilities, document_unigram_probabilities)\n",
    "    score_pv = (1 - args.lambda_) * score_qd + args.lambda_ * similarity\n",
    "    return score_pv, score_qd\n",
    "\n",
    "def get_probability_pv(word, document_idx, model):\n",
    "    word_vector = model.get_word_vector(word)\n",
    "    document_vector = document_vectors[document_idx]\n",
    "    similarity = model.similarity(word_vector, document_vector)\n",
    "    similarity_exponent = np.exp(similarity)\n",
    "    _sum = 0\n",
    "    for word in document_words[document_idx]: # model.vocabulary: # is this model vocabulary?\n",
    "        _sum += np.exp(model.similarity(document_vector, model.get_word_vector(word)))\n",
    "    probability_pv = similarity_exponent / _sum\n",
    "    return probability_pv\n",
    "\n",
    "def get_probability_wd(word, document_idx, document_unigram_probabilities, model):\n",
    "    probability_lm = document_unigram_probabilities[document_idx].get(word, 0)\n",
    "    return (1 - args.lambda_) * probability_lm + args.lambda_ * get_probability_pv(word, document_idx, model)\n",
    "\n",
    "def make_sparse_document_vectors(document_vectors, p=0.5):\n",
    "    ndim = document_vectors.shape[1]\n",
    "    min_ndim = list()\n",
    "    max_mdin = list()\n",
    "    for i in range(ndim):\n",
    "        min_ndim.append(np.min(document_vectors[:, i]))\n",
    "        max_mdin.append(np.max(document_vectors[:, i]))\n",
    "    a_min = np.mean(min_ndim)\n",
    "    a_max = np.mean(max_mdin)\n",
    "    t = (np.abs(a_min) + np.abs(a_max)) / 2\n",
    "    pt = p * t\n",
    "    document_vectors[np.abs(document_vectors) < pt] = 0\n",
    "    return document_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6860c718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fe8930e1c440cc90065ef9c74ebc4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc107caffc8452dbc03acb2fdf66559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scdv_document_vectors = [scdv_model.get_document_vector(word_tokenize(document)) for document in tqdm(documents)]\n",
    "scdv_document_vectors = np.asarray(scdv_document_vectors)\n",
    "scdv_document_vectors = make_sparse_document_vectors(scdv_document_vectors)\n",
    "\n",
    "baseline_document_vectors = [baseline_model.get_document_vector(word_tokenize(document)) for document in tqdm(documents)]\n",
    "baseline_document_vectors = np.asarray(baseline_document_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c11e6c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7240aaefb80d4fae8d181285ca831df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "document_lm = list()\n",
    "document_unigram_probabilities = list()\n",
    "for document in tqdm(document_words):\n",
    "    unigram_probabilities = dict()\n",
    "    train, vocab = padded_everygram_pipeline(3, [document])\n",
    "    lm = MLE(3)\n",
    "    lm.fit(train, vocab)\n",
    "    document_lm.append(lm)\n",
    "    for word in document:\n",
    "        unigram_probabilities[word] = lm.score(word)\n",
    "    document_unigram_probabilities.append(unigram_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad12112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b96a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efc7476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"India\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8052cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_results(query):\n",
    "    query_words = word_tokenize(query)\n",
    "    unigram_probabilities = dict()\n",
    "    train, vocab = padded_everygram_pipeline(1, [query])\n",
    "    lm = MLE(1)\n",
    "    lm.fit(train, vocab)\n",
    "    for word in query:\n",
    "        unigram_probabilities[word] = lm.score(word)\n",
    "        \n",
    "    query_vector_scdv = np.asarray(scdv_model.get_document_vector(query_words))\n",
    "    query_vector_baseline = np.asarray(baseline_model.get_document_vector(query_words))\n",
    "    \n",
    "    for document_idx in tqdm(range(total_documents)):\n",
    "\n",
    "            if (document_idx + 1) % 1000 == 0:\n",
    "                logging.info(f'{document_idx + 1}/{total_documents} documents processed')\n",
    "\n",
    "            score_lm = document_lm[document_idx].score(query_words[query_idx][-1], query_words[query_idx][:-1])\n",
    "            score_pv, score_qd = get_scores(model, query_idx, document_idx, query_unigram_probabilities, document_unigram_probabilities)\n",
    "            scores.append((score_pv, score_qd, score_lm, document_idx))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
